{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A dataset loader for imports85.data.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "STEPS = 1000\n",
    "PRICE_NORM_FACTOR = 1000\n",
    "train_fraction=0.7\n",
    "seed=100\n",
    "y_name=\"price\"\n",
    "\n",
    "try:\n",
    "  import pandas as pd  # pylint: disable=g-import-not-at-top\n",
    "except ImportError:\n",
    "  pass\n",
    "\n",
    "\n",
    "URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
    "\n",
    "# Order is important for the csv-readers, so we use an OrderedDict here.\n",
    "defaults = collections.OrderedDict([\n",
    "    (\"symboling\", [0]),\n",
    "    (\"normalized-losses\", [0.0]),\n",
    "    (\"make\", [\"\"]),\n",
    "    (\"fuel-type\", [\"\"]),\n",
    "    (\"aspiration\", [\"\"]),\n",
    "    (\"num-of-doors\", [\"\"]),\n",
    "    (\"body-style\", [\"\"]),\n",
    "    (\"drive-wheels\", [\"\"]),\n",
    "    (\"engine-location\", [\"\"]),\n",
    "    (\"wheel-base\", [0.0]),\n",
    "    (\"length\", [0.0]),\n",
    "    (\"width\", [0.0]),\n",
    "    (\"height\", [0.0]),\n",
    "    (\"curb-weight\", [0.0]),\n",
    "    (\"engine-type\", [\"\"]),\n",
    "    (\"num-of-cylinders\", [\"\"]),\n",
    "    (\"engine-size\", [0.0]),\n",
    "    (\"fuel-system\", [\"\"]),\n",
    "    (\"bore\", [0.0]),\n",
    "    (\"stroke\", [0.0]),\n",
    "    (\"compression-ratio\", [0.0]),\n",
    "    (\"horsepower\", [0.0]),\n",
    "    (\"peak-rpm\", [0.0]),\n",
    "    (\"city-mpg\", [0.0]),\n",
    "    (\"highway-mpg\", [0.0]),\n",
    "    (\"price\", [0.0])\n",
    "])  # pyformat: disable\n",
    "\n",
    "\n",
    "types = collections.OrderedDict((key, type(value[0])) for key, value in defaults.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\n",
      " 8192/25936 [========>.....................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "path = tf.contrib.keras.utils.get_file(URL.split(\"/\")[-1], URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it into a pandas dataframe\n",
    "df = pd.read_csv(path, names=types.keys(), dtype=types, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with unknowns\n",
    "data = df.dropna()\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(None)\n",
    "\n",
    "# Split the data into train/test subsets.\n",
    "x_train = data.sample(frac=train_fraction, random_state=seed)\n",
    "x_test = data.drop(x_train.index)\n",
    "\n",
    "# Extract the label from the features dataframe.\n",
    "y_train = x_train.pop(y_name)\n",
    "y_test = x_test.pop(y_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_no_question_marks(line):\n",
    "    \"\"\"Returns True if the line of text has no question marks.\"\"\"\n",
    "    # split the line into an array of characters\n",
    "    chars = tf.string_split(line[tf.newaxis], \"\").values\n",
    "    # for each character check if it is a question mark\n",
    "    is_question = tf.equal(chars, \"?\")\n",
    "    any_question = tf.reduce_any(is_question)\n",
    "    no_question = ~any_question\n",
    "    return no_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how the lines of the file should be parsed\n",
    "def decode_line(line):\n",
    "    \"\"\"Convert a csv line into a (features_dict,label) pair.\"\"\"\n",
    "    # Decode the line to a tuple of items based on the types of\n",
    "    # csv_header.values().\n",
    "    items = tf.decode_csv(line, list(defaults.values()))\n",
    "\n",
    "    # Convert the keys and items to a dict.\n",
    "    pairs = zip(defaults.keys(), items)\n",
    "    features_dict = dict(pairs)\n",
    "\n",
    "    # Remove the label from the features_dict\n",
    "    label = features_dict.pop(y_name)\n",
    "\n",
    "    return features_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = (tf.contrib.data.TextLineDataset(path).filter(has_no_question_marks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_training_set(line):\n",
    "    \"\"\"Returns a boolean tensor, true if the line is in the training set.\"\"\"\n",
    "    # If you randomly split the dataset you won't get the same split in both\n",
    "    # sessions if you stop and restart training later. Also a simple\n",
    "    # random split won't work with a dataset that's too big to `.cache()` as\n",
    "    # we are doing here.\n",
    "    num_buckets = 1000000\n",
    "    bucket_id = tf.string_to_hash_bucket_fast(line, num_buckets)\n",
    "    # Use the hash bucket id as a random number that's deterministic per example\n",
    "    return bucket_id < int(train_fraction * num_buckets)\n",
    "\n",
    "def in_test_set(line):\n",
    "    \"\"\"Returns a boolean tensor, true if the line is in the training set.\"\"\"\n",
    "    # Items not in the training set are in the test set.\n",
    "    # This line must use `~` instead of `not` beacuse `not` only works on python\n",
    "    # booleans but we are dealing with symbolic tensors.\n",
    "    return ~in_training_set(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (base_dataset.filter(in_training_set).cache().map(decode_line))\n",
    "# Take only the training-set lines.\n",
    "# Cache data so you only read the file once.\n",
    "# Decode each line into a (features_dict, label) pair.\n",
    "test = (base_dataset.filter(in_test_set).cache().map(decode_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the labels to units of thousands for better convergence.\n",
    "def to_thousands(features, labels):\n",
    "    return features, labels / PRICE_NORM_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(to_thousands)\n",
    "test = test.map(to_thousands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training input_fn.\n",
    "# Shuffling with a buffer larger than the data set ensures\n",
    "# that the examples are well mixed.\n",
    "# Repeat forever\n",
    "def input_train():\n",
    "    return (train.shuffle(1000).batch(128).repeat().make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the validation input_fn.\n",
    "def input_test():\n",
    "    return (test.shuffle(1000).batch(128).make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "      # \"curb-weight\" and \"highway-mpg\" are numeric columns.\n",
    "      tf.feature_column.numeric_column(key=\"curb-weight\"),\n",
    "      tf.feature_column.numeric_column(key=\"highway-mpg\"),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpzuh_x2jr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpzuh_x2jr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_is_chief': True, '_master': '', '_session_config': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpzuh_x2jr', '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1326ccef0>, '_save_checkpoints_steps': None, '_task_id': 0, '_log_step_count_steps': 100, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tf_random_seed': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 5, '_is_chief': True, '_master': '', '_session_config': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpzuh_x2jr', '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff1326ccef0>, '_save_checkpoints_steps': None, '_task_id': 0, '_log_step_count_steps': 100, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tf_random_seed': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpzuh_x2jr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpzuh_x2jr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18176.3, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18176.3, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.7102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 42.7102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1393.11, step = 101 (2.346 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1393.11, step = 101 (2.346 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1214.21, step = 201 (2.213 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1214.21, step = 201 (2.213 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1086.35, step = 301 (2.216 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1086.35, step = 301 (2.216 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 44.4904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 44.4904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 994.949, step = 401 (2.248 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 994.949, step = 401 (2.248 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.3581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.3581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 929.614, step = 501 (2.202 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 929.614, step = 501 (2.202 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 882.908, step = 601 (2.217 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 882.908, step = 601 (2.217 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 849.517, step = 701 (2.219 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 849.517, step = 701 (2.219 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.0328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 825.645, step = 801 (2.221 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 825.645, step = 801 (2.221 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 45.1945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 808.576, step = 901 (2.215 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 808.576, step = 901 (2.215 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpzuh_x2jr/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpzuh_x2jr/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 796.474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 796.474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-06-06:32:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-01-06-06:32:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzuh_x2jr/model.ckpt-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzuh_x2jr/model.ckpt-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-01-06-06:32:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-01-06-06:32:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 9.25865, global_step = 1000, loss = 453.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 9.25865, global_step = 1000, loss = 453.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "\n",
      "RMS error for the test set: 3043\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator.\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# Train the model.\n",
    "# By default, the Estimators log output every 100 steps.\n",
    "model.train(input_fn=input_train, steps=STEPS)\n",
    "\n",
    "# Evaluate how the model performs on data it has not yet seen.\n",
    "eval_result = model.evaluate(input_fn=input_test)\n",
    "\n",
    "# The evaluation returns a Python dictionary. The \"average_loss\" key holds the\n",
    "# Mean Squared Error (MSE).\n",
    "average_loss = eval_result[\"average_loss\"]\n",
    "\n",
    "# Convert MSE to Root Mean Square Error (RMSE).\n",
    "print(\"\\n\" + 80 * \"*\")\n",
    "print(\"\\nRMS error for the test set: ${:.0f}\".format(PRICE_NORM_FACTOR * average_loss**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model in prediction mode.\n",
    "input_dict = {\n",
    "  \"curb-weight\": np.array([2000, 3000]),\n",
    "  \"highway-mpg\": np.array([30, 40])\n",
    "}\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      input_dict, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction results:\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzuh_x2jr/model.ckpt-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzuh_x2jr/model.ckpt-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Curb weight:  2000lbs, Highway:  30mpg, Prediction: $  8695.01\n",
      "    Curb weight:  3000lbs, Highway:  40mpg, Prediction: $ 14073.10\n"
     ]
    }
   ],
   "source": [
    "predict_results = model.predict(input_fn=predict_input_fn)\n",
    "\n",
    "# Print the prediction results.\n",
    "print(\"\\nPrediction results:\")\n",
    "for i, prediction in enumerate(predict_results):\n",
    "    msg = (\"Curb weight: {: 4d}lbs, \"\n",
    "           \"Highway: {: 0d}mpg, \"\n",
    "           \"Prediction: ${: 9.2f}\")\n",
    "    msg = msg.format(input_dict[\"curb-weight\"][i], input_dict[\"highway-mpg\"][i],\n",
    "                     PRICE_NORM_FACTOR * prediction[\"predictions\"][0])\n",
    "\n",
    "    print(\"    \" + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
